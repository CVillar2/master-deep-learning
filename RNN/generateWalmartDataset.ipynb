{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "64fc5f66",
   "metadata": {},
   "source": [
    "# Practice 2.1 (Recurrent Neural Networks)\n",
    "\n",
    "Authors:\n",
    "\n",
    "1. Ovidio Manteiga Moar\n",
    "1. Carlos Villar Mart√≠nez"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9f23e4f8",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "For this first part of the practise we are using the Walmart Sales Dataset of 45 Stores. The file has information about the Weekly Sales of 45 stores for the year 2010-2012. With this information the goal is to create and train a model capable of predicting the sales of the next three weeks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95db380e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.layers import Input, Reshape, SimpleRNN, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "649b0883",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Returns a numpy array with size nrows x ncolumns-1. nrows and ncolums are the rows and columns of the dataset\n",
    "#the Date column is skipped (ncolumns-1)\n",
    "def readData(fname):\n",
    "    with open(fname) as f:\n",
    "        fileData = f.read()\n",
    "  \n",
    "    lines = fileData.split(\"\\n\")\n",
    "    header = lines[0].split(\",\")\n",
    "    lines = lines[1:] \n",
    "    #print(header) \n",
    "    #print(\"Data rows: \", len(lines))\n",
    "\n",
    "    rawData = np.zeros((len(lines), len(header)-1)) #skip the Date column\n",
    "\n",
    "    for i, aLine in enumerate(lines):       \n",
    "        splittedLine = aLine.split(\",\")[:]\n",
    "        rawData[i, 0] = splittedLine[0]\n",
    "        rawData[i, 1:] = [float(x) for x in splittedLine[2:]] \n",
    "\n",
    "    return rawData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4cedb758",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Returns the train and test data, normalized. It also returns the standard deviation of Weekly_Sales\n",
    "#Each list has a size equal to the number of stores\n",
    "#For each store there is a list of size trainNSaples (testNSamples) x nColums-1 (the store id is skipped)\n",
    "#Columns: Weekly_Sales,Holiday_Flag,Temperature,Fuel_Price,CPI,Unemployment\n",
    "def splitTrainTest(rawData, testPercent):\n",
    "\n",
    "    listStore = np.unique(rawData[:, 0])\n",
    "    trainNSamples = np.zeros(len(listStore))\n",
    "    \n",
    "    for i, storeId in enumerate(listStore):\n",
    "        trainNSamples[i] = np.count_nonzero(rawData[:, 0] == storeId)\n",
    "    trainNSamples = np.floor((1-testPercent) *  trainNSamples)\n",
    "\n",
    "    tmpTrain = np.zeros((int(np.sum(trainNSamples)), len(rawData[0])))\n",
    "\n",
    "    store = -1\n",
    "    counter = 0\n",
    "    counterTrain = 0\n",
    "    storeDict = dict(zip(listStore, trainNSamples))\n",
    "    for i, aLine in enumerate(rawData):\n",
    "        if store != aLine[0]:\n",
    "            store = int(aLine[0])\n",
    "            counter = 0\n",
    "        if(counter < storeDict.get(store)):\n",
    "            tmpTrain[counterTrain] = rawData[i][:]\n",
    "            counterTrain += 1\n",
    "            counter += 1\n",
    "\n",
    "    meanData = tmpTrain.mean(axis=0)\n",
    "    stdData = tmpTrain.std(axis=0)\n",
    "    rawNormData = (rawData - meanData) / stdData\n",
    "\n",
    "    allTrain = list()\n",
    "    allTest = list()\n",
    "    store = -1\n",
    "    counter = 0\n",
    "    for i, aLine in enumerate(rawNormData):\n",
    "        splittedLine = [float(x) for x in aLine[1:]] #skip store id\n",
    "        if store != rawData[i][0]:\n",
    "            if i != 0:\n",
    "                allTrain.append(storeDataTrain)\n",
    "                allTest.append(storeDataTest)\n",
    "            store = int(rawData[i][0])\n",
    "            storeDataTrain = list()\n",
    "            storeDataTest = list()\n",
    "            counter = 0\n",
    "\n",
    "        if(counter < storeDict.get(store)):\n",
    "            storeDataTrain.append(splittedLine)\n",
    "            counter += 1\n",
    "        else:\n",
    "            storeDataTest.append(splittedLine)\n",
    "\n",
    "        if i == len(rawNormData)-1:\n",
    "            allTrain.append(storeDataTrain)\n",
    "            allTest.append(storeDataTest)\n",
    "\n",
    "    return allTrain, allTest, stdData[1] #std of wSales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1eb8e1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#generates a time series given the input and ouput data, the sequence length and the batch size\n",
    "#seqLength is the number of weeks (observations) of data to be used as input\n",
    "#the target will be the weekly sales in 2 weeks\n",
    "def generateTimeSeries(data, wSales, seqLength, batchSize):   \n",
    "    sampling_rate = 1 #keep all the data points \n",
    "    weeksInAdvance = 3\n",
    "    delay = sampling_rate * (seqLength + weeksInAdvance - 1) #the target will be the weekly sales in 2 weeks\n",
    "    \n",
    "    dataset = keras.utils.timeseries_dataset_from_array(\n",
    "        data[:-delay],\n",
    "        targets=wSales[delay:],\n",
    "        sampling_rate=sampling_rate,\n",
    "        sequence_length=seqLength,\n",
    "        shuffle=True,\n",
    "        batch_size=batchSize,\n",
    "        start_index=0)\n",
    "    \n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d09807fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def printTimeSeriesList(theList):\n",
    "    print('list length', len(theList))\n",
    "    print('First element')\n",
    "    input, target = theList[0]\n",
    "    print([float(x) for x in input.numpy().flatten()], [float(x) for x in target.numpy().flatten()])\n",
    "    print('Last element')\n",
    "    input, target = theList[-1]\n",
    "    print([float(x) for x in input.numpy().flatten()], [float(x) for x in target.numpy().flatten()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bbeb5250",
   "metadata": {},
   "outputs": [],
   "source": [
    "#returns the training and test time series\n",
    "#it also returns the standard deviation of Weekly_Sales, and the number of input features\n",
    "def generateTrainTestData(fileName, testPercent, seqLength, batchSize):\n",
    "    rawData = readData(os.path.join(fileName))\n",
    "    allTrain, allTest, stdSales = splitTrainTest(rawData, testPercent)\n",
    "    \n",
    "    for i in range(len(allTrain)):\n",
    "        tmp_train = generateTimeSeries(np.array(allTrain[i]), np.array(allTrain[i])[:,0], seqLength, batchSize)\n",
    "        tmp_test = generateTimeSeries(np.array(allTest[i]), np.array(allTest[i])[:,0], seqLength, batchSize)\n",
    "\n",
    "        if i == 0:\n",
    "            train_dataset = tmp_train\n",
    "            test_dataset = tmp_test\n",
    "        else:\n",
    "            train_dataset = train_dataset.concatenate(tmp_train)\n",
    "            test_dataset = test_dataset.concatenate(tmp_test)\n",
    "    \n",
    "    return train_dataset, test_dataset, stdSales, np.shape(allTrain)[2]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "811d5da2",
   "metadata": {},
   "source": [
    "# The model\n",
    "\n",
    "In the following cells the starts the design of ur model. First of all we define the percentage of samples that are going to be part of the test set (as in the first practice we are going to use the test set as validation set), we decided to use a common split of 20% as test set and a 80% as training set. We also decided to change the sequence lenght from 8 to 12 as it gave us better results. After this we print the value of the sales standard deviation, as we will need it in the future in order to calculate the denormalized mae."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "dacc97c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "testPercent = 0.2\n",
    "seqLength = 12\n",
    "batchSize = 1\n",
    "nFeatures = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "0ae9ab0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STD(sales) = 571854.7800576452\n"
     ]
    }
   ],
   "source": [
    "trainData, testData, stdSales, nFeatures = generateTrainTestData(\"walmart-sales-dataset-of-45stores.csv\",\n",
    "    testPercent, seqLength, batchSize)\n",
    "print(f\"STD(sales) = {stdSales}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8290d3d3",
   "metadata": {},
   "source": [
    "The following cell is the most important one as all the layers of the model are ordered and defined there. We tried a lot of different things, for example using GRUs, RNNs or dropout, some of them obtained good results but te best one that we found was using a single LSTM layer.\n",
    "\n",
    "First of all we defined the input layer, in shape section we can write None instead of `seqLength` but, if all of our sequences have the same lenght it is recomended to specify the full shape as it may help to unlock some performance optimizations.\n",
    "\n",
    "After defining the input layer we present the LSTM layer, which is the recurrent layer of our model. The output of the last LSTM unit is the only one considered for the prediction, hence the `return_sequences` attribute is set to `False`. This is an example of a many to one RNN.\n",
    "\n",
    "Since the dimensionality of the output will be 128 (as the number of units), a dense layer with a single neuron is added to produce a single real value as output. It has only one output neuron that represents the prediction and, since it is a regression problem we do not need an activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "84d3bfd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = keras.Input(shape=(seqLength, nFeatures))\n",
    "x = layers.LSTM(128, activation='tanh', return_sequences=False)(inputs)\n",
    "outputs = layers.Dense(1)(x)\n",
    "model = keras.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "bdf3f108",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_205\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_209 (InputLayer)      [(None, 12, 6)]           0         \n",
      "                                                                 \n",
      " lstm_97 (LSTM)              (None, 128)               69120     \n",
      "                                                                 \n",
      " dense_205 (Dense)           (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 69,249\n",
      "Trainable params: 69,249\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "6adeae18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "4005/4005 [==============================] - 49s 11ms/step - loss: 0.0965 - mae: 0.1825 - val_loss: 0.2062 - val_mae: 0.3506\n",
      "Epoch 2/20\n",
      "4005/4005 [==============================] - 52s 13ms/step - loss: 0.0466 - mae: 0.1378 - val_loss: 0.1567 - val_mae: 0.3043\n",
      "Epoch 3/20\n",
      "4005/4005 [==============================] - 49s 12ms/step - loss: 0.0396 - mae: 0.1277 - val_loss: 0.1486 - val_mae: 0.2979\n",
      "Epoch 4/20\n",
      "4005/4005 [==============================] - 52s 13ms/step - loss: 0.0368 - mae: 0.1220 - val_loss: 0.1078 - val_mae: 0.2506\n",
      "Epoch 5/20\n",
      "4005/4005 [==============================] - 69s 17ms/step - loss: 0.0354 - mae: 0.1203 - val_loss: 0.0887 - val_mae: 0.2297\n",
      "Epoch 6/20\n",
      "4005/4005 [==============================] - 62s 15ms/step - loss: 0.0328 - mae: 0.1152 - val_loss: 0.0556 - val_mae: 0.1799\n",
      "Epoch 7/20\n",
      "4005/4005 [==============================] - 52s 13ms/step - loss: 0.0337 - mae: 0.1137 - val_loss: 0.0637 - val_mae: 0.1962\n",
      "Epoch 8/20\n",
      "4005/4005 [==============================] - 49s 12ms/step - loss: 0.0305 - mae: 0.1097 - val_loss: 0.0404 - val_mae: 0.1559\n",
      "Epoch 9/20\n",
      "4005/4005 [==============================] - 46s 12ms/step - loss: 0.0273 - mae: 0.1058 - val_loss: 0.0407 - val_mae: 0.1557\n",
      "Epoch 10/20\n",
      "4005/4005 [==============================] - 49s 12ms/step - loss: 0.0243 - mae: 0.1004 - val_loss: 0.0377 - val_mae: 0.1422\n",
      "Epoch 11/20\n",
      "4005/4005 [==============================] - 49s 12ms/step - loss: 0.0209 - mae: 0.0956 - val_loss: 0.0274 - val_mae: 0.1251\n",
      "Epoch 12/20\n",
      "4005/4005 [==============================] - 48s 12ms/step - loss: 0.0212 - mae: 0.0946 - val_loss: 0.0332 - val_mae: 0.1412\n",
      "Epoch 13/20\n",
      "4005/4005 [==============================] - 51s 13ms/step - loss: 0.0184 - mae: 0.0887 - val_loss: 0.0317 - val_mae: 0.1404\n",
      "Epoch 14/20\n",
      "4005/4005 [==============================] - 46s 12ms/step - loss: 0.0164 - mae: 0.0880 - val_loss: 0.0255 - val_mae: 0.1226\n",
      "Epoch 15/20\n",
      "4005/4005 [==============================] - 51s 13ms/step - loss: 0.0163 - mae: 0.0860 - val_loss: 0.0282 - val_mae: 0.1280\n",
      "Epoch 16/20\n",
      "4005/4005 [==============================] - 48s 12ms/step - loss: 0.0155 - mae: 0.0850 - val_loss: 0.0327 - val_mae: 0.1292\n",
      "Epoch 17/20\n",
      "4005/4005 [==============================] - 50s 12ms/step - loss: 0.0149 - mae: 0.0828 - val_loss: 0.0266 - val_mae: 0.1183\n",
      "Epoch 18/20\n",
      "4005/4005 [==============================] - 48s 12ms/step - loss: 0.0159 - mae: 0.0831 - val_loss: 0.0316 - val_mae: 0.1293\n",
      "Epoch 19/20\n",
      "4005/4005 [==============================] - 51s 13ms/step - loss: 0.0136 - mae: 0.0800 - val_loss: 0.0263 - val_mae: 0.1217\n",
      "Epoch 20/20\n",
      "4005/4005 [==============================] - 47s 12ms/step - loss: 0.0133 - mae: 0.0783 - val_loss: 0.0255 - val_mae: 0.1257\n"
     ]
    }
   ],
   "source": [
    "callbacks = [keras.callbacks.ModelCheckpoint(\"jena_gru.keras\")]\n",
    "model.compile(optimizer=\"rmsprop\", loss=\"mse\", metrics=[\"mae\"])\n",
    "history = model.fit(trainData, epochs=20, validation_data=testData, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "22ea0a86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Denormalized MAE = 67624.53608371985 (val_mae = 0.11825473606586456)\n"
     ]
    }
   ],
   "source": [
    "min_val_mae = min(history.history['val_mae'])\n",
    "std = stdSales\n",
    "denormalized_MAE = stdSales * min_val_mae\n",
    "print(f'Denormalized MAE = {denormalized_MAE} (val_mae = {min_val_mae})')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3f8aed30",
   "metadata": {},
   "source": [
    "In the previous cell we calculate the denormalized MAE using the minimum validation MAE result obtained for the validation set, since that model snapshot is stored with the callback and it is the one to choose."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0b285b37",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "\n",
    "In this first part of the practice, we have worked with something quite different from what we worked in the previous practice, we stopped working with images to start working with a dataset containing data from sales and try to predict those sales. This is a problem that needs a complete different approach. We were also asked to use at least one recurrent layer which was not asked in the previous practice. \n",
    "\n",
    "First of all, we had to try a lot of different models to find the one that offered the best results, we found it when we tried a LSTM layer with the configuration that can be seen in the model section. We also realised that for this dataset a dropout or another kind of regularization layer did not worked really well, the same happened when we tried to stack several recurrent layers (even though we found some promising results with those configurations). \n",
    "\n",
    "We also tried different sequence length values and we found that with bigger ones we obtained better results, being 12 the most reasonable one. The single layer GRU with the same number of units could also be considered for performance reasons, since it performed close to the LSTM.\n",
    "\n",
    "Another thing that we tried were different activation functions and optimizers, but without any relevant improvements.\n",
    "\n",
    "Increasing the batch size provides better results and faster compared to 1, which causes a lot of fluctuation within each epic and a slower convergence. We used the batching for evaluating different models although from the description of the assignment we understood that parameter could not be changed in the final model evaluation.\n",
    "\n",
    "Adding RNN layers could help in problems where there are more complex relationships between the features and the output, but in this case, after testing with multiple configurations, it works worse in general, so a simpler model is preferred.\n",
    "\n",
    "Finally, the best result obtained was a val_mae that oscilated in values of 0,11 and 0,12 which is translated into values of approximately 67000 for the denormalized MAE. In the practice statement, it is said that a good result is a denormalized MAE of 68000 or below, so we can say that our result is quite satisfying.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d9fc4d82",
   "metadata": {},
   "source": [
    "# Appendix: model/hyperparameters performance comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "5e754086",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(seqLength, batchSize):\n",
    "    testPercent = 0.2\n",
    "    nFeatures = 6\n",
    "    csv = \"walmart-sales-dataset-of-45stores.csv\"\n",
    "    trainData, testData, stdSales, nFeatures = generateTrainTestData(csv, testPercent, seqLength, batchSize)\n",
    "    return trainData, testData, stdSales, nFeatures\n",
    "\n",
    "def create_RNN(units, layer, seqLength, nFeatures):\n",
    "    inputs = keras.Input(shape=(seqLength, nFeatures))\n",
    "    x = layer(units, activation='tanh', return_sequences=False)(inputs)\n",
    "    outputs = layers.Dense(1)(x)\n",
    "    model = keras.Model(inputs, outputs)\n",
    "    name = \"1-layer RNN ({layer_type}) with {units} units\".format(layer_type=layer.__name__, units=units)\n",
    "    return model, name\n",
    "\n",
    "def create_double_RNN(units, layer, seqLength, nFeatures):\n",
    "    inputs = keras.Input(shape=(seqLength, nFeatures))\n",
    "    x = layer(units, activation='tanh', return_sequences=True)(inputs)\n",
    "    x = layer(units//2, activation='tanh', return_sequences=False)(x)\n",
    "    outputs = layers.Dense(1)(x)\n",
    "    model = keras.Model(inputs, outputs)\n",
    "    name = \"2-layer RNN ({layer_type}) with {units} units\".format(layer_type=layer.__name__, units=units)\n",
    "    return model, name\n",
    "\n",
    "def create_simpleRNN(units, seqLength, nFeatures):\n",
    "    return create_RNN(units, layers.SimpleRNN, seqLength, nFeatures)\n",
    "\n",
    "def create_double_simpleRNN(units, seqLength, nFeatures):\n",
    "    return create_double_RNN(units, layers.SimpleRNN, seqLength, nFeatures)\n",
    "\n",
    "def create_GRU(units, seqLength, nFeatures):\n",
    "    return create_RNN(units, layers.GRU, seqLength, nFeatures)\n",
    "\n",
    "def create_double_GRU(units, seqLength, nFeatures):\n",
    "    return create_double_RNN(units, layers.GRU, seqLength, nFeatures)\n",
    "\n",
    "def create_LSTM(units, seqLength, nFeatures):\n",
    "    return create_RNN(units, layers.LSTM, seqLength, nFeatures)\n",
    "\n",
    "def create_double_LSTM(units, seqLength, nFeatures):\n",
    "    return create_double_RNN(units, layers.LSTM, seqLength, nFeatures)\n",
    "\n",
    "def train_and_evaluate(model, name, trainData, testData, stdSales, epochs=5, verbose=1):\n",
    "    # Train\n",
    "    model.compile(optimizer=\"rmsprop\", loss=\"mse\", metrics=[\"mae\"])\n",
    "    history = model.fit(trainData, epochs=epochs, validation_data=testData, verbose=verbose)\n",
    "    # Evaluate\n",
    "    min_val_mae = min(history.history['val_mae'])\n",
    "    std = stdSales\n",
    "    denormalized_MAE = stdSales * min_val_mae\n",
    "    print(f'Model = {name}')\n",
    "    print(f'Denormalized MAE = {denormalized_MAE} (val_mae = {min_val_mae})')\n",
    "    return name, denormalized_MAE, min_val_mae\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "8677e381",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-layer RNN (SimpleRNN) with 128 units\n",
      "Epoch 1/5\n",
      "90/90 [==============================] - 8s 79ms/step - loss: 0.3257 - mae: 0.3877 - val_loss: 0.1449 - val_mae: 0.3297\n",
      "Epoch 2/5\n",
      "90/90 [==============================] - 6s 61ms/step - loss: 0.1322 - mae: 0.2690 - val_loss: 0.0472 - val_mae: 0.1734\n",
      "Epoch 3/5\n",
      "90/90 [==============================] - 6s 65ms/step - loss: 0.0961 - mae: 0.2300 - val_loss: 0.0377 - val_mae: 0.1461\n",
      "Epoch 4/5\n",
      "90/90 [==============================] - 6s 65ms/step - loss: 0.0737 - mae: 0.2037 - val_loss: 0.0398 - val_mae: 0.1618\n",
      "Epoch 5/5\n",
      "90/90 [==============================] - 6s 64ms/step - loss: 0.0622 - mae: 0.1839 - val_loss: 0.0411 - val_mae: 0.1561\n",
      "Model = 1-layer RNN (SimpleRNN) with 128 units\n",
      "Denormalized MAE = 83566.3465094285 (val_mae = 0.14613211154937744)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('1-layer RNN (SimpleRNN) with 128 units',\n",
       " 83566.3465094285,\n",
       " 0.14613211154937744)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate data\n",
    "trainData, testData, stdSales, nFeatures = generate_data(seqLength=12, batchSize=64)\n",
    "# Create model\n",
    "model, name = create_simpleRNN(128, 12, nFeatures)\n",
    "print(name)\n",
    "# Train model\n",
    "train_and_evaluate(model, name, trainData, testData, stdSales, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "317242a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model = 1-layer RNN (SimpleRNN) with 32 units(length = 4)\n",
      "Denormalized MAE = 120342.58268036958 (val_mae = 0.21044255793094635)\n",
      "Model = 2-layer RNN (SimpleRNN) with 32 units(length = 4)\n",
      "Denormalized MAE = 111683.50151940108 (val_mae = 0.19530045986175537)\n",
      "Model = 1-layer RNN (GRU) with 32 units(length = 4)\n",
      "Denormalized MAE = 76374.08788951811 (val_mae = 0.13355503976345062)\n",
      "Model = 2-layer RNN (GRU) with 32 units(length = 4)\n",
      "Denormalized MAE = 78637.1577692012 (val_mae = 0.1375124603509903)\n",
      "Model = 1-layer RNN (LSTM) with 32 units(length = 4)\n",
      "Denormalized MAE = 79341.6733105584 (val_mae = 0.13874444365501404)\n",
      "Model = 2-layer RNN (LSTM) with 32 units(length = 4)\n",
      "Denormalized MAE = 76848.96291024641 (val_mae = 0.13438545167446136)\n",
      "Model = 1-layer RNN (SimpleRNN) with 64 units(length = 4)\n",
      "Denormalized MAE = 104328.1196486688 (val_mae = 0.18243813514709473)\n",
      "Model = 2-layer RNN (SimpleRNN) with 64 units(length = 4)\n",
      "Denormalized MAE = 123425.25678268241 (val_mae = 0.21583321690559387)\n",
      "Model = 1-layer RNN (GRU) with 64 units(length = 4)\n",
      "Denormalized MAE = 71964.11049525686 (val_mae = 0.1258433312177658)\n",
      "Model = 2-layer RNN (GRU) with 64 units(length = 4)\n",
      "Denormalized MAE = 76368.19114974019 (val_mae = 0.13354472815990448)\n",
      "Model = 1-layer RNN (LSTM) with 64 units(length = 4)\n",
      "Denormalized MAE = 69006.95088508115 (val_mae = 0.12067215889692307)\n",
      "Model = 2-layer RNN (LSTM) with 64 units(length = 4)\n",
      "Denormalized MAE = 83688.857243225 (val_mae = 0.1463463455438614)\n",
      "Model = 1-layer RNN (SimpleRNN) with 128 units(length = 4)\n",
      "Denormalized MAE = 112461.17242346557 (val_mae = 0.1966603696346283)\n",
      "Model = 2-layer RNN (SimpleRNN) with 128 units(length = 4)\n",
      "Denormalized MAE = 123689.36596285131 (val_mae = 0.216295063495636)\n",
      "Model = 1-layer RNN (GRU) with 128 units(length = 4)\n",
      "Denormalized MAE = 63061.89959535246 (val_mae = 0.11027607321739197)\n",
      "Model = 2-layer RNN (GRU) with 128 units(length = 4)\n",
      "Denormalized MAE = 73785.20609450436 (val_mae = 0.12902787327766418)\n",
      "Model = 1-layer RNN (LSTM) with 128 units(length = 4)\n",
      "Denormalized MAE = 70378.80353477375 (val_mae = 0.1230711117386818)\n",
      "Model = 2-layer RNN (LSTM) with 128 units(length = 4)\n",
      "Denormalized MAE = 81823.17820103228 (val_mae = 0.1430838406085968)\n",
      "Model = 1-layer RNN (SimpleRNN) with 32 units(length = 8)\n",
      "Denormalized MAE = 84630.21380392833 (val_mae = 0.14799249172210693)\n",
      "Model = 2-layer RNN (SimpleRNN) with 32 units(length = 8)\n",
      "Denormalized MAE = 115746.67903579841 (val_mae = 0.20240572094917297)\n",
      "Model = 1-layer RNN (GRU) with 32 units(length = 8)\n",
      "Denormalized MAE = 68618.12395434917 (val_mae = 0.1199922189116478)\n",
      "Model = 2-layer RNN (GRU) with 32 units(length = 8)\n",
      "Denormalized MAE = 79181.07236461261 (val_mae = 0.1384636014699936)\n",
      "Model = 1-layer RNN (LSTM) with 32 units(length = 8)\n",
      "Denormalized MAE = 71017.036142081 (val_mae = 0.12418718636035919)\n",
      "Model = 2-layer RNN (LSTM) with 32 units(length = 8)\n",
      "Denormalized MAE = 82834.39238124342 (val_mae = 0.1448521465063095)\n",
      "Model = 1-layer RNN (SimpleRNN) with 64 units(length = 8)\n",
      "Denormalized MAE = 106782.22003751581 (val_mae = 0.18672960996627808)\n",
      "Model = 2-layer RNN (SimpleRNN) with 64 units(length = 8)\n",
      "Denormalized MAE = 94655.82180192875 (val_mae = 0.16552422940731049)\n",
      "Model = 1-layer RNN (GRU) with 64 units(length = 8)\n",
      "Denormalized MAE = 63695.27506151315 (val_mae = 0.11138365417718887)\n",
      "Model = 2-layer RNN (GRU) with 64 units(length = 8)\n",
      "Denormalized MAE = 66462.49914960797 (val_mae = 0.11622268706560135)\n",
      "Model = 1-layer RNN (LSTM) with 64 units(length = 8)\n",
      "Denormalized MAE = 67614.50651331725 (val_mae = 0.1182371973991394)\n",
      "Model = 2-layer RNN (LSTM) with 64 units(length = 8)\n",
      "Denormalized MAE = 79091.24081730215 (val_mae = 0.13830651342868805)\n",
      "Model = 1-layer RNN (SimpleRNN) with 128 units(length = 8)\n",
      "Denormalized MAE = 81488.78533634273 (val_mae = 0.14249908924102783)\n",
      "Model = 2-layer RNN (SimpleRNN) with 128 units(length = 8)\n",
      "Denormalized MAE = 131895.02873713893 (val_mae = 0.23064427077770233)\n",
      "Model = 1-layer RNN (GRU) with 128 units(length = 8)\n",
      "Denormalized MAE = 67871.77735002982 (val_mae = 0.11868708580732346)\n",
      "Model = 2-layer RNN (GRU) with 128 units(length = 8)\n",
      "Denormalized MAE = 74337.70163927469 (val_mae = 0.12999401986598969)\n",
      "Model = 1-layer RNN (LSTM) with 128 units(length = 8)\n",
      "Denormalized MAE = 74933.45982545037 (val_mae = 0.13103581964969635)\n",
      "Model = 2-layer RNN (LSTM) with 128 units(length = 8)\n",
      "Denormalized MAE = 95867.3461872838 (val_mae = 0.16764281690120697)\n",
      "Model = 1-layer RNN (SimpleRNN) with 32 units(length = 12)\n",
      "Denormalized MAE = 88208.14587718448 (val_mae = 0.15424920618534088)\n",
      "Model = 2-layer RNN (SimpleRNN) with 32 units(length = 12)\n",
      "Denormalized MAE = 124294.96625082396 (val_mae = 0.21735407412052155)\n",
      "Model = 1-layer RNN (GRU) with 32 units(length = 12)\n",
      "Denormalized MAE = 59337.874089838275 (val_mae = 0.1037638857960701)\n",
      "Model = 2-layer RNN (GRU) with 32 units(length = 12)\n",
      "Denormalized MAE = 64370.83096394655 (val_mae = 0.11256499588489532)\n",
      "Model = 1-layer RNN (LSTM) with 32 units(length = 12)\n",
      "Denormalized MAE = 76338.99717505932 (val_mae = 0.13349367678165436)\n",
      "Model = 2-layer RNN (LSTM) with 32 units(length = 12)\n",
      "Denormalized MAE = 73245.83335213 (val_mae = 0.1280846744775772)\n",
      "Model = 1-layer RNN (SimpleRNN) with 64 units(length = 12)\n",
      "Denormalized MAE = 76596.9710190431 (val_mae = 0.1339447945356369)\n",
      "Model = 2-layer RNN (SimpleRNN) with 64 units(length = 12)\n",
      "Denormalized MAE = 76015.86094800942 (val_mae = 0.13292860984802246)\n",
      "Model = 1-layer RNN (GRU) with 64 units(length = 12)\n",
      "Denormalized MAE = 57195.93023265989 (val_mae = 0.10001827776432037)\n",
      "Model = 2-layer RNN (GRU) with 64 units(length = 12)\n",
      "Denormalized MAE = 59238.055578626474 (val_mae = 0.10358933359384537)\n",
      "Model = 1-layer RNN (LSTM) with 64 units(length = 12)\n",
      "Denormalized MAE = 62164.24026236759 (val_mae = 0.10870634019374847)\n",
      "Model = 2-layer RNN (LSTM) with 64 units(length = 12)\n",
      "Denormalized MAE = 75288.03112914876 (val_mae = 0.13165585696697235)\n",
      "Model = 1-layer RNN (SimpleRNN) with 128 units(length = 12)\n",
      "Denormalized MAE = 95897.45194109216 (val_mae = 0.16769546270370483)\n",
      "Model = 2-layer RNN (SimpleRNN) with 128 units(length = 12)\n",
      "Denormalized MAE = 94930.58260741904 (val_mae = 0.1660047024488449)\n",
      "Model = 1-layer RNN (GRU) with 128 units(length = 12)\n",
      "Denormalized MAE = 57353.9108787766 (val_mae = 0.10029453784227371)\n",
      "Model = 2-layer RNN (GRU) with 128 units(length = 12)\n",
      "Denormalized MAE = 63405.146091009155 (val_mae = 0.11087630689144135)\n",
      "Model = 1-layer RNN (LSTM) with 128 units(length = 12)\n",
      "Denormalized MAE = 71000.9521878457 (val_mae = 0.1241590604186058)\n",
      "Model = 2-layer RNN (LSTM) with 128 units(length = 12)\n",
      "Denormalized MAE = 91855.64584573967 (val_mae = 0.1606275737285614)\n",
      "BEST MODEL\n",
      "(57195.93023265989, '1-layer RNN (GRU) with 64 units(length = 12)', 12)\n"
     ]
    }
   ],
   "source": [
    "def create_all_models(units, seqLength, nFeatures):\n",
    "    models = []\n",
    "    models += [create_simpleRNN(units, seqLength, nFeatures)]\n",
    "    models += [create_double_simpleRNN(units, seqLength, nFeatures)]\n",
    "    models += [create_GRU(units, seqLength, nFeatures)]\n",
    "    models += [create_double_GRU(units, seqLength, nFeatures)]\n",
    "    models += [create_LSTM(units, seqLength, nFeatures)]\n",
    "    models += [create_double_LSTM(units, seqLength, nFeatures)]\n",
    "    return models\n",
    "\n",
    "bestMAE = 1.0\n",
    "best = None\n",
    "for length in [4, 8, 12]:\n",
    "    trainData, testData, stdSales, nFeatures = generate_data(seqLength=length, batchSize=64)\n",
    "    for units in [32, 64, 128]:\n",
    "        models = create_all_models(units, length, nFeatures=6)\n",
    "        for model in models:\n",
    "            modelName = model[1] + \"(length = {length})\".format(length=length)\n",
    "            name, dMAE, valMAE = train_and_evaluate(model[0], modelName, trainData, testData, stdSales, epochs=5, verbose=0)\n",
    "            if valMAE < bestMAE:\n",
    "                bestMAE = valMAE\n",
    "                best = (dMAE, name, length)\n",
    "\n",
    "print(\"BEST MODEL\")\n",
    "print(best)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
