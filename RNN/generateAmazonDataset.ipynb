{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fa94409e",
   "metadata": {},
   "source": [
    "# Practice 2.2 (Recurrent Neural Networks)\n",
    "\n",
    "Authors:\n",
    "\n",
    "1. Ovidio Manteiga Moar\n",
    "1. Carlos Villar Mart√≠nez\n",
    "\n",
    "\n",
    "# Introduction\n",
    "\n",
    "## Dataset\n",
    "\n",
    "For the second part of the RNN assignment, we will use the Amazon Reviews for Sentiment Analysis (Kaggle dataset). This dataset consists of a few million Amazon customer reviews (input text) and star ratings (output labels). \n",
    "\n",
    "The classes are `__label__1` and `__label__2`, and there is only one class per row. `__label__1` corresponds to 1-star and 2-star reviews, and `__label__2` corresponds to 4-star and 5-star reviews. 3-star reviews (i.e. reviews with neutral sentiment) were not included in the original dataset. Most of the reviews are in English, but there are a few in other languages, like Spanish. The original dataset has 3,600,000 examples for training and 400,000 for testing. We will use a reduced version of the dataset, with 25,000 examples for training and 25,000 examples for testing.\n",
    "\n",
    "The function `readData` in this notebook reads the dataset (train and test) and the `transformData` function transforms the text yielding the preprocessed train and test sets to use. The transformed datasets represents the texts as sequences of integers representing each word based on a vocabulary using the Keras function `TextVectorization`. It requires two hyperparameters:\n",
    "\n",
    "- The size of the vocabulary (maxFeatures).\n",
    "- The maximum length of the text (seqLength). By default, seqLength has \n",
    "been set to the average length of the training samples plus two times their standard deviation. \n",
    "\n",
    "\n",
    "## Problem\n",
    "\n",
    "Given the dataset described above, the problem is to predict the correct label indicating the sentiment (positive `__label__2` or negative `__label__1`) of given a review as a text. This is an instance of a binary classification problem, where the inputs are the texts with the reviews and the outputs the labels indicating the sentiment.\n",
    "\n",
    "The problem is to be tackled using some kind of RNNs, which should be able to capture some of the meaning in the reviews that determines if a review is considered positive or negative.\n",
    "\n",
    "\n",
    "## Evaluation\n",
    "\n",
    "The metric to evaluate the performance of the models will be the *accuracy* achieved in the test set provided. In the implementation we treat the test set as the validation set to get the value of the metric after each epoch of training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b6e3b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "#import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d2a09fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reads a file. Each line has the format: label text\n",
    "#Returns a list with the text and a list with the labels\n",
    "def readData(fname):\n",
    "\n",
    "    with open(fname, 'r', encoding=\"utf-8\") as f:\n",
    "        fileData = f.read()\n",
    "  \n",
    "    lines = fileData.split(\"\\n\")\n",
    "    textData = list()\n",
    "    textLabel = list()\n",
    "    lineLength = np.zeros(len(lines))\n",
    "    \n",
    "    for i, aLine in enumerate(lines):     \n",
    "        if not aLine:\n",
    "            break  \n",
    "        label = aLine.split(\" \")[0]\n",
    "        lineLength[i] = len(aLine.split(\" \"))\n",
    "        if(label == \"__label__1\"):\n",
    "            textLabel.append(0)\n",
    "            textData.append(aLine.lstrip(\"__label__1 \"))\n",
    "\n",
    "        elif(label == \"__label__2\"):\n",
    "            textLabel.append(1)\n",
    "            textData.append(aLine.lstrip(\"__label__2 \"))\n",
    "\n",
    "        else:\n",
    "            print(\"\\nError in readData: \", i, aLine)\n",
    "            exit()\n",
    "    \n",
    "    f.close()\n",
    "    return textData, textLabel, int(np.average(lineLength)+2*np.std(lineLength))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2731ee70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformData(x_train, y_train, x_test, y_test, maxFeatures, seqLength):\n",
    "    #transforms text input to int input based on the vocabulary\n",
    "    #max_tokens = maxFeatures is the size of the vocabulary\n",
    "    #output_sequence_length =  seqLength is the maximum length of the transformed text. Adds 0 is text length is shorter\n",
    "    precLayer = layers.experimental.preprocessing.TextVectorization(max_tokens = maxFeatures, \n",
    "    standardize =  'lower_and_strip_punctuation', split = 'whitespace', output_mode = 'int', \n",
    "    output_sequence_length =  seqLength)\n",
    "    precLayer.adapt(x_train)\n",
    "    #print(precLayer.get_vocabulary())\n",
    "    x_train_int = precLayer(x_train)\n",
    "    y_train = tf.convert_to_tensor(y_train)\n",
    "    #print(x_train_int)\n",
    "    #print(y_train)\n",
    "    x_test_int= precLayer(x_test)\n",
    "    y_test = tf.convert_to_tensor(y_test)\n",
    "    #print(x_test_int)\n",
    "    #print(y_test)\n",
    "\n",
    "    return x_train_int, y_train, x_test_int, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "db189834",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, seqLength = readData(\"./amazon/train_small.txt\")\n",
    "x_test, y_test, tmp = readData(\"./amazon/test_small.txt\")\n",
    "\n",
    "# Hyperparameters\n",
    "maxFeatures = 1000\n",
    "embedding_dim = 64\n",
    "seqLength = seqLength * 2\n",
    "\n",
    "x_train_int, y_train, x_test_int, y_test = transformData(x_train, y_train, x_test, y_test, maxFeatures, seqLength)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "80129625",
   "metadata": {},
   "source": [
    "# The model\n",
    "\n",
    "In the following cell, it is defined the model that achieved the best performance, with more than 88% accuracy in the test set. It consists of an embedding layer mapping the vectorized sequences of words into vectors representing their meaning, followed by a recurrent layer of GRUs with 64 units, and finally the output layer as a dense layer with a single unit and a sigmoid activation to produce the binary ouput. The GRU is configured to return only the output of the last cell to be used to predict the output label (`return_sequences=False`), so that the type of recurrent architecture is many-to-one. Since the dimensionality of the output will be 64 (as the number of units), a dense layer with a single neuron and a sigmoid activation is added to produce a single binary value as output.\n",
    "\n",
    "First of all we defined the input layer, in shape section we can write `None` instead of `seqLength` but, if all of our sequences have the same length it is recomended to specify the full shape as it may help to unlock some performance optimizations.\n",
    "\n",
    "The embedding model has the input dimension as the number of features (representing the number of words in the vocabulary), the output dimension as the specified length of the vectors that it will produce for each word, and the input length as the length of the sequences of words. Also, the parameter `mask_zero` was set to true, so that the zeroes in the sequences (which appear as padding) are not considered to train the embedding layer nor subsequently in the recurrent layers.\n",
    "\n",
    "As a baseline to compare, without an RNN, only with a dense layer of 64 units after the embedding layer, it achieves a 84% accuracy in the test set (with a 100% accuracy in the train set).\n",
    "```\n",
    "Epoch 10/20\n",
    "196/196 [==============================] - 8s 43ms/step - loss: 0.0019 - accuracy: 0.9999 - val_loss: 0.8515 - val_accuracy: 0.8401\n",
    "```\n",
    "\n",
    "We tried many different models, for example using Simple RNNs, GRUs, LSTMs, bidirectional LSTMs with multiple configurations (single or multiple layers), but none of them worked better and the GRU with 64 units was the simpler model we found that achieved the best accuracy in the test set around 88%.\n",
    "\n",
    "We also experimented with different regularization techniques like dropout, batch normalization, L1/L2 regularization in the recurrent layers, but none of them outperformed the single GRU layer, whose test accuracy stagnates but does not drop considerably.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1fce8e99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_10\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_12 (InputLayer)       [(None, 332)]             0         \n",
      "                                                                 \n",
      " embedding_11 (Embedding)    (None, 332, 64)           64000     \n",
      "                                                                 \n",
      " gru_7 (GRU)                 (None, 64)                24960     \n",
      "                                                                 \n",
      " dense_19 (Dense)            (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 89,025\n",
      "Trainable params: 89,025\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_shape = (seqLength)\n",
    "inputs = keras.Input(shape=input_shape)\n",
    "x = layers.Embedding(input_dim=maxFeatures,\n",
    "                     output_dim=embedding_dim,\n",
    "                     input_length=seqLength, \n",
    "                     mask_zero=True)(inputs)\n",
    "x = layers.GRU(64, activation='tanh', return_sequences=False)(x)\n",
    "outputs = layers.Dense(1, activation='sigmoid')(x)\n",
    "model = keras.Model(inputs, outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "19d32de5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "196/196 [==============================] - 173s 828ms/step - loss: 0.4776 - accuracy: 0.7576 - val_loss: 0.3515 - val_accuracy: 0.8504\n",
      "Epoch 2/20\n",
      "196/196 [==============================] - 159s 814ms/step - loss: 0.3367 - accuracy: 0.8604 - val_loss: 0.3413 - val_accuracy: 0.8584\n",
      "Epoch 3/20\n",
      "196/196 [==============================] - 170s 868ms/step - loss: 0.3198 - accuracy: 0.8675 - val_loss: 0.3356 - val_accuracy: 0.8589\n",
      "Epoch 4/20\n",
      "196/196 [==============================] - 180s 922ms/step - loss: 0.2902 - accuracy: 0.8804 - val_loss: 0.3029 - val_accuracy: 0.8733\n",
      "Epoch 5/20\n",
      "196/196 [==============================] - 179s 915ms/step - loss: 0.2650 - accuracy: 0.8942 - val_loss: 0.2913 - val_accuracy: 0.8797\n",
      "Epoch 6/20\n",
      "196/196 [==============================] - 166s 848ms/step - loss: 0.2461 - accuracy: 0.9003 - val_loss: 0.2882 - val_accuracy: 0.8816\n",
      "Epoch 7/20\n",
      "196/196 [==============================] - 146s 746ms/step - loss: 0.2380 - accuracy: 0.9048 - val_loss: 0.2893 - val_accuracy: 0.8792\n",
      "Epoch 8/20\n",
      "196/196 [==============================] - 160s 817ms/step - loss: 0.2268 - accuracy: 0.9073 - val_loss: 0.3038 - val_accuracy: 0.8749\n",
      "Epoch 9/20\n",
      "196/196 [==============================] - 150s 768ms/step - loss: 0.2208 - accuracy: 0.9120 - val_loss: 0.2998 - val_accuracy: 0.8748\n",
      "Epoch 10/20\n",
      "196/196 [==============================] - 150s 767ms/step - loss: 0.2124 - accuracy: 0.9153 - val_loss: 0.3060 - val_accuracy: 0.8769\n",
      "Epoch 11/20\n",
      "196/196 [==============================] - 150s 767ms/step - loss: 0.2073 - accuracy: 0.9178 - val_loss: 0.3083 - val_accuracy: 0.8748\n",
      "Epoch 12/20\n",
      "196/196 [==============================] - 152s 779ms/step - loss: 0.2014 - accuracy: 0.9206 - val_loss: 0.3206 - val_accuracy: 0.8746\n",
      "Epoch 13/20\n",
      "196/196 [==============================] - 156s 796ms/step - loss: 0.1950 - accuracy: 0.9240 - val_loss: 0.3211 - val_accuracy: 0.8700\n",
      "Epoch 14/20\n",
      "196/196 [==============================] - 161s 821ms/step - loss: 0.1893 - accuracy: 0.9252 - val_loss: 0.3358 - val_accuracy: 0.8708\n",
      "Epoch 15/20\n",
      "196/196 [==============================] - 139s 710ms/step - loss: 0.1825 - accuracy: 0.9293 - val_loss: 0.3413 - val_accuracy: 0.8711\n",
      "Epoch 16/20\n",
      "196/196 [==============================] - 155s 794ms/step - loss: 0.1749 - accuracy: 0.9320 - val_loss: 0.3462 - val_accuracy: 0.8685\n",
      "Epoch 17/20\n",
      "196/196 [==============================] - 135s 688ms/step - loss: 0.1697 - accuracy: 0.9348 - val_loss: 0.3667 - val_accuracy: 0.8695\n",
      "Epoch 18/20\n",
      "196/196 [==============================] - 137s 702ms/step - loss: 0.1618 - accuracy: 0.9388 - val_loss: 0.3562 - val_accuracy: 0.8654\n",
      "Epoch 19/20\n",
      "196/196 [==============================] - 135s 692ms/step - loss: 0.1579 - accuracy: 0.9398 - val_loss: 0.3737 - val_accuracy: 0.8658\n",
      "Epoch 20/20\n",
      "196/196 [==============================] - 134s 686ms/step - loss: 0.1515 - accuracy: 0.9417 - val_loss: 0.3675 - val_accuracy: 0.8663\n"
     ]
    }
   ],
   "source": [
    "callbacks = [ keras.callbacks.ModelCheckpoint(\"jena_gru_amazon.keras\") ]\n",
    "model.compile(optimizer=\"adam\", loss='binary_crossentropy', metrics=[\"accuracy\"])\n",
    "history = model.fit(x_train_int, \n",
    "                    y_train, epochs=20,\n",
    "                    batch_size=128, \n",
    "                    validation_data=(x_test_int, y_test), \n",
    "                    callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e8ef205c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAX TEST ACC = 88.16%\n"
     ]
    }
   ],
   "source": [
    "max_val_accuracy = max(history.history['val_accuracy'])\n",
    "print(\"MAX TEST ACC = {mva:.2f}%\".format(mva=max_val_accuracy*100))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b8601518",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "\n",
    "1. The hyperparameters that worked better were a vocabulary size of 1000, a sequence length of double the default and an embedding dimension of 64. The increased vocabulary size and sequence length allows to capture more information about the meaning of the words and texts.\n",
    "1. The models that performed better were the single-layer RNN models in general, and among those the GRUs of 64 units, which are also preferrable for simplicity.\n",
    "1. None of the regularization techniques applied (dropout, batch normalization, L1/L2 regularization) improved the performance of the model, which does not clearly overfit, but keeps increasing slightly the train accuracy while the validation accuracy plateaus around the maximum. This can be due to the significant size of the dataset.\n",
    "1. None of the more complex models with multiple RNN layers and even multiple dense layer worked better. Some performed similarly, but the preference is for simpler models. Moreover when the training times are significantly longer for complex recurrent models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0293495",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
